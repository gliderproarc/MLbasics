# First ML Livebook

```elixir
Mix.install(
  [
    {:kino_bumblebee, "~> 0.1.0"},
    {:exla, "~> 0.4.1"}
  ],
  config: [nx: [default_backend: EXLA.Backend]]
)
```

## Intro

```elixir
IO.puts("Let's get this party started")
```

Looks like this is working and ready to go. Next is to see if I can get a model running.

<!-- livebook:{"attrs":{"compiler":"exla","max_new_tokens":100,"min_new_tokens":null,"sequence_length":100,"task_id":"text_generation","variant_id":"gpt2_medium"},"chunks":[[0,330],[332,502]],"kind":"Elixir.KinoBumblebee.TaskCell","livebook_object":"smart_cell"} -->

```elixir
{:ok, model_info} = Bumblebee.load_model({:hf, "gpt2-medium"}, log_params_diff: false)
{:ok, tokenizer} = Bumblebee.load_tokenizer({:hf, "gpt2-medium"})

serving =
  Bumblebee.Text.generation(model_info, tokenizer,
    max_new_tokens: 100,
    compile: [batch_size: 1, sequence_length: 100],
    defn_options: [compiler: EXLA]
  )

text_input = Kino.Input.textarea("Text", default: "Yesterday, I was reading a book and")
form = Kino.Control.form([text: text_input], submit: "Run")
frame = Kino.Frame.new()

form
|> Kino.Control.stream()
|> Kino.listen(fn %{data: %{text: text}} ->
  Kino.Frame.render(frame, Kino.Markdown.new("Running..."))
  %{results: [%{text: generated_text}]} = Nx.Serving.run(serving, text)
  Kino.Frame.render(frame, Kino.Markdown.new(generated_text))
end)

Kino.Layout.grid([form, frame], boxed: true, gap: 16)
```

Well there we go. It's working now.

<!-- livebook:{"break_markdown":true} -->

Nice. I can type in blocks wile other blocks are running long processes.

Getting a model running was very easy once I decided to give up running inside docker. If I can get a docker image made with the compilers all ready to go, I can imagine I could run ML stuff in Docker. But while I am just getting used to things, it's much easier to use the LiveBook app.

Also cool, I can put my computer's GPU to good use with training once I get to that stage. And apparently models once they are on my machine shoudl run on the TPU (not 100% sure).

Okay, got some output from a GPT2 model.... haha. Not very close. But not incomprehensible.
